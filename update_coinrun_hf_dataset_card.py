"""
update_coinrun_hf_dataset_card.py

Generate (and optionally upload) a Hugging Face dataset card (README.md) for the
CoinRun agent-episode datasets produced by `generate_coinrun_data.py`.

It summarizes local `metadata_worker_*.json` files and documents how to read the
`.array_record` shards.

Example:

python3 update_coinrun_hf_dataset_card.py \
  --dataset-dir /ephemeral/datasets/coinrun_hard_agent_episodes \
  --repo-id edwhu/coinrun_hard_agent \
  --upload
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


@dataclass
class SplitStats:
    episodes: int = 0
    successful: int = 0
    avg_len_weighted_sum: float = 0.0  # weighted by episodes

    def add(self, episodes: int, successful: int, avg_len: float) -> None:
        self.episodes += int(episodes)
        self.successful += int(successful)
        self.avg_len_weighted_sum += float(avg_len) * int(episodes)

    @property
    def success_rate(self) -> float:
        return (self.successful / self.episodes * 100.0) if self.episodes > 0 else 0.0

    @property
    def avg_len(self) -> float:
        return (self.avg_len_weighted_sum / self.episodes) if self.episodes > 0 else 0.0


def _read_json(path: Path) -> Dict[str, Any]:
    with path.open("r") as f:
        return json.load(f)


def _count_shards(dataset_dir: Path) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for split in ["train", "val", "test"]:
        split_dir = dataset_dir / split
        if not split_dir.exists():
            out[split] = 0
            continue
        out[split] = sum(1 for _ in split_dir.glob("*.array_record"))
    return out


def _summarize_metadata(dataset_dir: Path) -> Tuple[Dict[str, Any], Dict[str, SplitStats], int]:
    meta_paths = sorted(dataset_dir.glob("metadata*.json"))
    if not meta_paths:
        raise SystemExit(f"No metadata*.json found in: {dataset_dir}")

    first = _read_json(meta_paths[0])
    num_workers = int(first.get("num_workers", len(meta_paths)))

    stats = {
        "train": SplitStats(),
        "val": SplitStats(),
        "test": SplitStats(),
    }

    for mp in meta_paths:
        m = _read_json(mp)
        stats["train"].add(
            episodes=int(m.get("num_episodes_train", 0)),
            successful=int(m.get("successful_episodes_train", 0)),
            avg_len=float(m.get("avg_episode_len_train", 0.0)),
        )
        stats["val"].add(
            episodes=int(m.get("num_episodes_val", 0)),
            successful=int(m.get("successful_episodes_val", 0)),
            avg_len=float(m.get("avg_episode_len_val", 0.0)),
        )
        stats["test"].add(
            episodes=int(m.get("num_episodes_test", 0)),
            successful=int(m.get("successful_episodes_test", 0)),
            avg_len=float(m.get("avg_episode_len_test", 0.0)),
        )

    return first, stats, num_workers


def _render_readme(
    repo_id: str,
    dataset_dir: Path,
    first_meta: Dict[str, Any],
    split_stats: Dict[str, SplitStats],
    shard_counts: Dict[str, int],
) -> str:
    # Pull out a few fields if present; keep robust to missing keys.
    wandb_run_path = first_meta.get("wandb_run_path")
    algo = first_meta.get("algo") or first_meta.get("algo_name") or first_meta.get("algo")
    num_actions = first_meta.get("num_actions")
    chunk_size = first_meta.get("chunk_size")
    chunks_per_file = first_meta.get("chunks_per_file")
    frame_layout = first_meta.get("frame_layout", "HWC_uint8")
    indexing = first_meta.get("indexing", "dreamer_t+1")
    min_episode_length = first_meta.get("min_episode_length")
    max_episode_length = first_meta.get("max_episode_length")

    def fmt_opt(x: Any) -> str:
        return "unknown" if x is None else str(x)

    # HF dataset card frontmatter. Keep it conservative.
    frontmatter = "\n".join(
        [
            "---",
            "tags:",
            "- reinforcement-learning",
            "- procgen",
            "- coinrun",
            "task_categories:",
            "- reinforcement-learning",
            "---",
            "",
        ]
    )

    summary_lines = [
        frontmatter,
        f"# {repo_id}",
        "",
        "## Dataset Summary",
        "",
        "CoinRun (Procgen) episodes generated by rolling out a pretrained RL agent and saving trajectories",
        "as chunked `.array_record` shards (compatible with the DreamerV3/DreamerV4-style dataset format used",
        "in `dreamer4-jax-private/coinrun_data/generate_coinrun_dataset.py`).",
        "",
        "The Hub viewer will not automatically decode `.array_record`; see **How to read the data** below.",
        "",
        "## Contents",
        "",
        "The repository layout is:",
        "",
        "- `train/` – `.array_record` shards",
        "- `val/` – `.array_record` shards",
        "- `test/` – `.array_record` shards",
        "- `metadata_worker_*.json` – generation metadata for each rollout worker",
        "",
        "## Key stats (from metadata)",
        "",
        "| split | episodes | successful | success_rate | avg_episode_len | shards |",
        "|---|---:|---:|---:|---:|---:|",
    ]

    for split in ["train", "val", "test"]:
        s = split_stats[split]
        summary_lines.append(
            f"| {split} | {s.episodes} | {s.successful} | {s.success_rate:.2f}% | {s.avg_len:.2f} | {shard_counts.get(split, 0)} |"
        )

    summary_lines += [
        "",
        "## Generation details",
        "",
        f"- **Local source folder**: `{dataset_dir}` (this is where it was uploaded from)",
        f"- **Algorithm**: `{fmt_opt(algo)}`",
        f"- **W&B run path**: `{fmt_opt(wandb_run_path)}`",
        f"- **Actions**: `{fmt_opt(num_actions)}` (discrete)",
        f"- **Chunking**: `chunk_size={fmt_opt(chunk_size)}`, `chunks_per_file={fmt_opt(chunks_per_file)}`",
        f"- **Episode length filters**: `min_episode_length={fmt_opt(min_episode_length)}`, `max_episode_length={fmt_opt(max_episode_length)}`",
        f"- **Frame layout**: `{fmt_opt(frame_layout)}`",
        f"- **Indexing convention**: `{fmt_opt(indexing)}`",
        "",
        "## Data format",
        "",
        "Each `.array_record` file contains many *chunk records*. Each record is a Python `pickle` of a dict with:",
        "",
        "- `raw_video`: bytes of a contiguous `uint8` array representing frames in **HWC** order",
        "- `sequence_length`: int, number of frames in this chunk",
        "- `actions`: (optional) array of actions aligned with Dreamer-style indexing",
        "- `rewards`: (optional) array of rewards aligned with Dreamer-style indexing",
        "",
        "### Dreamer-style indexing (`dreamer_t+1`)",
        "",
        "These chunks are stored with the convention:",
        "",
        "- states/frames: \(s_0, s_1, ..., s_T\) (so there are **T+1** frames)",
        "- actions: first entry is a dummy value, then real actions \(a_0..a_{T-1}\)",
        "- rewards: first entry is `NaN`, then real rewards \(r_0..r_{T-1}\)",
        "",
        "## How to read the data",
        "",
        "You’ll need the `array_record` Python package plus `pickle`/`numpy`. Example:",
        "",
        "```python",
        "import pickle",
        "import numpy as np",
        "from array_record.python.array_record_module import ArrayRecordReader",
        "",
        "path = 'train/w00_data_000000.array_record'",
        "reader = ArrayRecordReader(path)",
        "",
        "# Read the first record in the file",
        "record_bytes = reader.read(0)",
        "ex = pickle.loads(record_bytes)",
        "",
        "seq_len = int(ex['sequence_length'])",
        "video = np.frombuffer(ex['raw_video'], dtype=np.uint8)",
        "",
        "# If you know H/W/C you can reshape (CoinRun is typically 64x64x3):",
        "# video = video.reshape(seq_len, 64, 64, 3)",
        "",
        "actions = ex.get('actions')",
        "rewards = ex.get('rewards')",
        "print(seq_len, video.shape, None if actions is None else actions.shape)",
        "```",
        "",
        "## Notes",
        "",
        "- If you re-upload from the same local folder using `upload_large_folder()`, do not delete the local",
        "  `.cache/.huggingface/` unless you want to force a fresh upload resume state.",
    ]

    return "\n".join(summary_lines).strip() + "\n"


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-dir", required=True, type=str)
    parser.add_argument("--repo-id", required=True, type=str)
    parser.add_argument("--repo-type", default="dataset", choices=["dataset", "model", "space"])
    parser.add_argument(
        "--output",
        default=None,
        help="Where to write the README.md locally (default: <dataset-dir>/README.md).",
    )
    parser.add_argument("--upload", action="store_true", help="Upload README.md to the Hub repo.")
    parser.add_argument(
        "--verify-remote",
        action="store_true",
        help="After upload, list repo files and print shard counts for train/val/test.",
    )
    args = parser.parse_args()

    dataset_dir = Path(args.dataset_dir).expanduser().resolve()
    first_meta, split_stats, _ = _summarize_metadata(dataset_dir)
    shard_counts = _count_shards(dataset_dir)

    readme = _render_readme(
        repo_id=str(args.repo_id),
        dataset_dir=dataset_dir,
        first_meta=first_meta,
        split_stats=split_stats,
        shard_counts=shard_counts,
    )

    out_path = Path(args.output).expanduser().resolve() if args.output else (dataset_dir / "README.md")
    out_path.write_text(readme)
    print(f"Wrote README to: {out_path}")

    if args.upload:
        from huggingface_hub import HfApi

        api = HfApi()
        commit_info = api.upload_file(
            path_or_fileobj=str(out_path),
            path_in_repo="README.md",
            repo_id=str(args.repo_id),
            repo_type=str(args.repo_type),
            commit_message="Add dataset card (README.md)",
        )
        print("Uploaded README.md")
        print("commit_url=", getattr(commit_info, "commit_url", None))

    if args.verify_remote:
        from huggingface_hub import HfApi

        api = HfApi()
        files = api.list_repo_files(repo_id=str(args.repo_id), repo_type=str(args.repo_type))
        for split in ["train/", "val/", "test/"]:
            n = sum(f.startswith(split) and f.endswith(".array_record") for f in files)
            print(f"{split}*.array_record {n}")


if __name__ == "__main__":
    main()
